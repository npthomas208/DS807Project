---
title: |
  | Final Project
  | DS 807: Modeling Unstructured Data
author: |
  | Anthony Morin, Nate Thomas
output: html_document
---

```{r}
library(jpeg)
library(ggplot2)
library(EnvStats)
library(MASS)
library(dbscan)
library(factoextra)
library(gam)
library(flexmix)
library(keras)
library(tensorflow)
library(dplyr)
library(stringr)
library(tfdatasets)
```

## Data Requirements:

- You can pick any type of data that satisfies **at least one** of the following criteria:

    1. Text Data
    2. Image Data
    3. Unsupervised Data


- Some sources are:

    - Kaggle <https://www.kaggle.com/datasets>
    - UCI Machine Learning Repository <https://archive.ics.uci.edu/ml/index.php>
    
- Read your data in R.

```{r}
bluejay <- readJPEG("Images/Test/OIP.jfif")
dm=dim(bluejay)
```

## The grading rubric can be found below:

+----------------+---------------+--------------------+-----------------------+
|                | R code        | Decision/Why       | Communication         |
|                |               |                    |  of findings          |
+================+===============+====================+=======================+
| Percentage of  | 30%           | 35%                | 35%                   |
| Assigned Points|               |                    |                       |
+----------------+---------------+--------------------+-----------------------+


- **Decision/why?**: Explain your reasoning behind your choice of the procedure, set of variables and such for the question. 

    - Explain why you use the procedure/model/variables
    - To exceed this criterion, describe steps taken to implement the procedure in a non technical way.


- **Communication of your findings**: Explain your results.

    - Explain why you think one model is better than the other.
    - To exceed this criterion, explain your model in a non technical way.

## Note

- Since there is a great range of potential data, the instructions are written in a general way. If some steps do not make sense in your case, please reach out to verify.

## Part 1: Exploratory Data Analysis

1. Explain the purpose of the analysis *for your data*, i.e., what are you trying to achieve?

We have selected a blue jay image for analysis.  We will be analyzing this for color clustering, and endeavoring to locate and identify the blue jay with as little external information. 

First, the image:

```{r}
rgbImage <- data.frame(
  x=rep(1:dm[2], each=dm[1]),
  y=rep(dm[1]:1, dm[2]),
  r.value=as.vector(bluejay[,,1]),
  g.value=as.vector(bluejay[,,2]),
  b.value=as.vector(bluejay[,,3]))

p_0 <- ggplot(data=rgbImage, aes(x=x, y=y, col=rgb(r.value,g.value,b.value))) + 
    ggtitle("bluejay")+
    geom_point() + 
    scale_color_identity()

p_0
```
2. Check for existence of NA's (missing data)

Performed explicitly:
```{r}
length(which(complete.cases(bluejay)))
dim(bluejay)[1]*dim(bluejay)[2]*dim(bluejay)[3]
``` 
As can be seen above, the image is read in with each pixel defined.  This is verified since the number of cases is equal to the multiplied dimensionality: `y * x * n`. Where `x` is the x dimension of image array; `y` is the y dimension of the image, and `n` is the three color values: red, green, and blue [r,g,b].

3. Use appropriate plots for EDA, i.e., word counts for text data.

```{r}
p_1 <- ggplot(rgbImage, aes(x=r.value))+
  geom_histogram(bins = 100)
p_1
```
```{r}
p_2 <- ggplot(rgbImage, aes(x=g.value))+
  geom_histogram(bins = 100)
p_2

```

```{r}
p_3 <- ggplot(rgbImage, aes(x=r.value))+
  geom_histogram(bins = 100)
p_3
```

As investigation, since we are looking at an image of a blue jay it may be of value to look only at the blue hue (isolation performed by zeroing `r.value`, and `g.value`)

```{r}
rgbImage_blued <- data.frame(
  x=rep(1:dm[2], each=dm[1]),
  y=rep(dm[1]:1, dm[2]),
  r.value=as.vector(rep(0,length(bluejay[,,1]))),
  g.value=as.vector(rep(0,length(bluejay[,,2]))),
  b.value=as.vector(bluejay[,,3]))

p_4 <- ggplot(data=rgbImage_blued, aes(x=x, y=y, col=rgb(r.value,g.value,b.value))) + 
    ggtitle("bluejay")+
    geom_point() + 
    scale_color_identity()

p_4
```
```{r}
rgbImage_blued$x2 <- rgbImage_blued$x^2
rgbImage_blued$y2 <- rgbImage_blued$y^2
rgbImage_blued$x3 <- rgbImage_blued$x^3
rgbImage_blued$y3 <- rgbImage_blued$y^3



lmxy <- lm(b.value~x+y+x2+y2+x3+y3, data = rgbImage_blued)

rgbImage_blued$x[which.max(predict(lmxy,newdata=rgbImage_blued))]
rgbImage_blued$y[which.max(predict(lmxy,newdata=rgbImage_blued))]

rgbImage_blued_max <- data.frame(
  x=rep(1:dm[2], each=dm[1]),
  y=rep(dm[1]:1, dm[2]),
  r.value=as.vector(rep(0,length(bluejay[,,1]))),
  g.value=as.vector(rep(0,length(bluejay[,,2]))),
  b.value=as.vector(bluejay[,,3]))

rgbImage_blued_max$r.value[which.max(predict(lmxy,newdata=rgbImage_blued))] = 1

p_5 <- ggplot(data=rgbImage_blued_max, aes(x=x, y=y, col=rgb(r.value,g.value,b.value))) + 
    ggtitle("bluejay")+
    geom_point() + 
    scale_color_identity()

p_5
```

```{r}
smooth.spline(rgbImage$x,rgbImage$r.value)
smooth.spline(rgbImage$y,rgbImage$r.value)

rdf <- mean(smooth.spline(rgbImage$x,rgbImage$r.value)$df,smooth.spline(rgbImage$y,rgbImage$r.value)$df)

smooth.spline(rgbImage$x,rgbImage$g.value)
smooth.spline(rgbImage$y,rgbImage$g.value)

gdf <- mean(smooth.spline(rgbImage$x,rgbImage$g.value)$df,smooth.spline(x = rgbImage$y,rgbImage$g.value)$df)

smooth.spline(rgbImage$x,rgbImage$b.value)
smooth.spline(rgbImage$y,rgbImage$b.value)

bdf <- mean(smooth.spline(rgbImage$x,rgbImage$b.value)$df,smooth.spline(rgbImage$y,rgbImage$b.value)$df)

gam_red <- gam(r.value~s(x,df=rdf,spar=1)+s(y,df=rdf,spar=1),data=rgbImage)
gam_green <- gam(g.value~s(x,df=gdf,spar=1)+s(y,df=gdf,spar=1),data=rgbImage)
gam_blue <- gam(b.value~s(x,df=bdf,spar=1)+s(y,df=bdf,spar=1),data=rgbImage)
rgbImage$r.value_ss <- predict(gam_red, newdata=rgbImage)
rgbImage$g.value_ss <- predict(gam_green, newdata=rgbImage)
rgbImage$b.value_ss <- predict(gam_blue, newdata=rgbImage)

rgbImage$b.value_ss[which(rgbImage$b.value_ss<0)] <- 0

min(rgbImage$r.value_ss)
max(rgbImage$r.value_ss)

min(rgbImage$g.value_ss)
max(rgbImage$g.value_ss)

min(rgbImage$b.value_ss)
max(rgbImage$b.value_ss)

p_6 <- ggplot(data=rgbImage, aes(x=x, y=y, col=rgb(r.value_ss,g.value_ss,b.value_ss))) + 
    ggtitle("bluejay - gone")+
    geom_point() + 
    scale_color_identity()

p_6
```


```{r}
eps <- 0.2
rgbImage$r.value_ss_gone <- ifelse((abs(rgbImage[3] - rgbImage[6])>eps) | (abs(rgbImage[4] - rgbImage[7])>eps) | (abs(rgbImage[5] - rgbImage[8])>eps),0,rgbImage$r.value_ss)
rgbImage$g.value_ss_gone <- ifelse((abs(rgbImage[3] - rgbImage[6])>eps) | (abs(rgbImage[4] - rgbImage[7])>eps) | (abs(rgbImage[5] - rgbImage[8])>eps),0,rgbImage$g.value_ss)
rgbImage$b.value_ss_gone <- ifelse((abs(rgbImage[3] - rgbImage[6])>eps) | (abs(rgbImage[4] - rgbImage[7])>eps) | (abs(rgbImage[5] - rgbImage[8])>eps),0,rgbImage$b.value_ss)

p_7 <- ggplot(data=rgbImage, aes(x=x, y=y, col=rgb(r.value_ss_gone,g.value_ss_gone,b.value_ss_gone))) + 
    ggtitle("bluejay - gone, smoothing spline")+
    geom_point() + 
    scale_color_identity()

p_7
```
```{r}
eps <- 0.3
rgbImage$r.value_ss_gone <- ifelse((abs(rgbImage[3] - rgbImage[6])>eps) | (abs(rgbImage[4] - rgbImage[7])>eps) | (abs(rgbImage[5] - rgbImage[8])>eps),0,rgbImage$r.value_ss)
rgbImage$g.value_ss_gone <- ifelse((abs(rgbImage[3] - rgbImage[6])>eps) | (abs(rgbImage[4] - rgbImage[7])>eps) | (abs(rgbImage[5] - rgbImage[8])>eps),0,rgbImage$g.value_ss)
rgbImage$b.value_ss_gone <- ifelse((abs(rgbImage[3] - rgbImage[6])>eps) | (abs(rgbImage[4] - rgbImage[7])>eps) | (abs(rgbImage[5] - rgbImage[8])>eps),0,rgbImage$b.value_ss)

p_8 <- ggplot(data=rgbImage, aes(x=x, y=y, col=rgb(r.value_ss_gone,g.value_ss_gone,b.value_ss_gone))) + 
    ggtitle("bluejay - gone, smoothing spline")+
    geom_point() + 
    scale_color_identity()

p_8
```


```{r}
eps <- 0.4
rgbImage$r.value_ss_gone <- ifelse((abs(rgbImage[3] - rgbImage[6])>eps) | (abs(rgbImage[4] - rgbImage[7])>eps) | (abs(rgbImage[5] - rgbImage[8])>eps),0,rgbImage$r.value_ss)
rgbImage$g.value_ss_gone <- ifelse((abs(rgbImage[3] - rgbImage[6])>eps) | (abs(rgbImage[4] - rgbImage[7])>eps) | (abs(rgbImage[5] - rgbImage[8])>eps),0,rgbImage$g.value_ss)
rgbImage$b.value_ss_gone <- ifelse((abs(rgbImage[3] - rgbImage[6])>eps) | (abs(rgbImage[4] - rgbImage[7])>eps) | (abs(rgbImage[5] - rgbImage[8])>eps),0,rgbImage$b.value_ss)

p_9 <- ggplot(data=rgbImage, aes(x=x, y=y, col=rgb(r.value_ss_gone,g.value_ss_gone,b.value_ss_gone))) + 
    ggtitle("bluejay - gone, smoothing spline")+
    geom_point() + 
    scale_color_identity()

p_9
```


```{r}
imagedata=rgbImage[,3:5]

fviz_nbclust(imagedata, kmeans, method = "gap_stat", k.max=4, nboot = 2)
```

```{r}
image_c = kmeans(imagedata, 1)
image_c$centers

```

```{r}
colours <- rgb(image_c$centers[image_c$cluster, ])

p_10 <- ggplot(data=rgbImage, aes(x=x, y=y, col=colours)) + 
    ggtitle("bluejay - kmeans k=1")+
    geom_point() + 
    scale_color_identity()

p_10
```

```{r}
image_c2 = kmeans(imagedata, 2)
image_c2$centers

```

```{r}
colours2 <- rgb(image_c2$centers[image_c2$cluster, ])

p_11 <- ggplot(data=rgbImage, aes(x=x, y=y, col=colours2)) + 
    ggtitle("bluejay - kmeans k=2")+
    geom_point() + 
    scale_color_identity()

p_11
```

4. Do you need to scale your data or do you need dimension reduction? If so, perform a principle components analysis on a scaled, or not-scaled data depending on your needs.

Scaling is unnecessary since the `rgb` values are all scaled `[0,1]`.  

## Part 2: Clustering (20 points)

1. Develop a clustering algorithm for your data: Choose from topic models, k-means, k-medoids, hierarchical, or DBSCAN. 

```{r}
eps = 10^c(-5:5)
minPts = 2^c(1:7)
hyper_grid <- expand.grid(eps = eps, minPts = minPts)
for (i in 1:nrow(hyper_grid)) {
  temp=dbscan(rgbImage[,3:5],eps=hyper_grid$eps[i],minPts = hyper_grid$minPts[i])
  if (any(temp$cluster>2) && all(temp$cluster<300)){
    c_db<-temp
    break
  }
}
```

```{r}
max(c_db$cluster)

removeZeros <- function(v){
  v[v !=0]
}

clustvector2rgb <- function(cluster){
  r_ <- rep(0,length(cluster))
  g_ <- rep(0,length(cluster))
  b_ <- rep(0,length(cluster))
  
  for (i in 1:max(cluster)){
    c1 = cluster == i
    if (is.nan(mean(removeZeros(as.vector(bluejay[,,1]*c1))))){
      rtemp = 0
    }
    else {
      rtemp = mean(removeZeros(as.vector(bluejay[,,1]*c1)))
    }
    if (is.nan(mean(removeZeros(as.vector(bluejay[,,2]*c1))))){
      gtemp = 0
    }
    else {
      gtemp = mean(removeZeros(as.vector(bluejay[,,2]*c1)))
    }
    if(is.nan(mean(removeZeros(as.vector(bluejay[,,3]*c1))))){
      btemp = 0
    }
    else {
      btemp = mean(removeZeros(as.vector(bluejay[,,3]*c1)))
    }
    r_ <- r_ + rtemp*c1
    g_ <- g_ + gtemp*c1
    b_ <- b_ + btemp*c1
    if (any(is.nan(r_))){
      print(which(is.nan(r_)))
      print(i)
      break
    }
  }
  return(cbind(r_,g_,b_))
}

r_g_b_ <- clustvector2rgb(c_db$cluster)
r_ <- r_g_b_[,1]
g_ <- r_g_b_[,2]
b_ <- r_g_b_[,3]

rgbImage_dbscan <- data.frame(
  x=rep(1:dm[2], each=dm[1]),
  y=rep(dm[1]:1, dm[2]),
  r.value=as.vector(r_),
  g.value=as.vector(g_),
  b.value=as.vector(b_))

p_12 <- ggplot(data=rgbImage_dbscan, aes(x=x, y=y, col=rgb(r.value,g.value,b.value))) + 
    ggtitle("bluejay - dbscan")+
    geom_point() + 
    scale_color_identity()

p_12
```

```{r}
eps = c(0.005,0.01,0.02,0.04,0.08)
minPts = c(3,4,5,6,7)
hyper_grid <- expand.grid(eps = eps, minPts = minPts)
for (i in 1:nrow(hyper_grid)) {
  temp=dbscan(rgbImage[,3:5],eps=hyper_grid$eps[i],minPts = hyper_grid$minPts[i])
  if (any(temp$cluster>2) && all(temp$cluster<300)){
    c_db1<-temp
    break
  }
}
```

```{r}
max(c_db1$cluster)
table(c_db1$cluster)

r_g_b_ <- clustvector2rgb(c_db1$cluster)
r_ <- r_g_b_[,1]
g_ <- r_g_b_[,2]
b_ <- r_g_b_[,3]

rgbImage_dbscan <- data.frame(
  x=rep(1:dm[2], each=dm[1]),
  y=rep(dm[1]:1, dm[2]),
  r.value=as.vector(r_),
  g.value=as.vector(g_),
  b.value=as.vector(b_) )


p_13 <- ggplot(data=rgbImage_dbscan, aes(x=x, y=y, col=rgb(r.value,g.value,b.value))) + 
    ggtitle("bluejay - dbscan")+
    geom_point() + 
    scale_color_identity()

p_13
```


2. Explain your choices on model parameters, i.e. k, eps, minpts, and communicate your results.

## Part 3: Mixture Models (20 points)

1. Apply a mixture model based clustering to your data.

2. Explain your choices on model parameters, and communicate your results.

```{r}
rgbImage_mat <- as.matrix(rgbImage[3:5])

model_fm = stepFlexmix(rgbImage_mat~1, k=1:4, nrep=5,
                     model=FLXMCmvnorm(),
                     control=list(tolerance=1e-15, iter.max=1000))

best_fit=getModel(model_fm, which = "BIC")
prior(best_fit)
best_fit@cluster

r_g_b_ <- clustvector2rgb(best_fit@cluster)
r_ <- r_g_b_[,1]
g_ <- r_g_b_[,2]
b_ <- r_g_b_[,3]

rgbImage_dbscan <- data.frame(
  x=rep(1:dm[2], each=dm[1]),
  y=rep(dm[1]:1, dm[2]),
  r.value=as.vector(r_),
  g.value=as.vector(g_),
  b.value=as.vector(b_) )


p_14 <- ggplot(data=rgbImage_dbscan, aes(x=x, y=y, col=rgb(r.value,g.value,b.value))) + 
    ggtitle("bluejay - flexmix, normal")+
    geom_point() + 
    scale_color_identity()

p_14
#
```

## Part 4: Deep Learning (20 points)

1. Apply a type of neural network algorithm to your data.

```{r}
use_condaenv("r-tensorflow")

nn = list()


nn$csv <- as.list(read.delim("C:/UNH/GitHub/UNH/DS807/DS807Project/Images/Train/train.csv") %>%
  mutate(class = as.factor(class)))

nn$train=list()
nn$train$y = array(as.numeric(nn$csv$class, levels(nn$csv$class)), dim = c(length(nn$csv$file), 1))

nn$csv$image = rep(NA, length(nn$csv$file))

for(i in 1:length(nn$csv$file))
{
  nn$csv$image[i] = list(as.integer(outer(readJPEG(paste("Images/Train/", nn$csv$file[i], sep="")),  255)))
}

nn$tarray <- function(x) aperm(x, seq_along(dim(x)))
nn$train$x = nn$tarray(array(unlist(nn$csv$image), c(length(nn$csv$image),1024, 1024, 3)))

nn = list(train = nn$train)

```

```{r}
nn$modelCNN = keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_shape = c(1024,2024, 2)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu")

summary(nn$modelCNN)
```


```{r}
nn$modelDense = nn$modelCNN %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")

summary(nn$modelDense)
```

```{r}
nn$model = nn$modelDense %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = "accuracy"
)
```

```{r}
history <- nn$model %>% 
  keras::fit(
    x = nn$train$x, y = nn$train$y,
    epochs = 10,
    validation_split = 0.15,
    verbose = 2
  )
```

2. Explain your choices on model parameters, and communicate your results.


## Part 5: Conclusion (20 points)

1. (10 points) Based on the purpose of your analysis stated in Part 1, which analysis did a good/better/satisfactory job? How do you think you can improve the analysis?

    
2. (10 points) What are your learning outcomes for this assignment? Please focus on your learning outcomes in terms of analysis, model interpretations, and R skills - it is up to you to include this part in your presentation or not.


